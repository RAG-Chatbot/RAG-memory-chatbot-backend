import torch
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline
from backend.retrieval import RAGRetriever
from backend.memory_manager import MemoryManager
from duckduckgo_search import DDGS  # âœ… Web Search Integration

# âœ… Lightweight model for fast CPU execution
MODEL_NAME = "google/flan-t5-large"

class Chatbot:
    def __init__(self):
        print("ğŸ”„ Initializing Chatbot...")

        # Initialize Memory & RAG Retriever
        self.memory = MemoryManager()
        self.retriever = RAGRetriever()

        # Load Tokenizer & Model
        print(f"ğŸ“¥ Loading Model: {MODEL_NAME}")
        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
        self.model = AutoModelForSeq2SeqLM.from_pretrained(
            MODEL_NAME,
            torch_dtype=torch.float32,  # Ensures CPU compatibility
            device_map="cpu"  # Forces CPU execution
        )

        self.llm = pipeline("text2text-generation", model=self.model, tokenizer=self.tokenizer)

        print("âœ… Chatbot Ready!")

    def duckduckgo_search(self, query):
        """Fetch top DuckDuckGo search result for queries without strong memory match."""
        try:
            with DDGS() as ddgs:
                results = list(ddgs.text(query, max_results=3))
            
            if results:
                return "\n".join([f"- {res['body']}" for res in results if 'body' in res])
            return None  # No results found
        except Exception as e:
            print(f"âš ï¸ Web search error: {e}")
            return None

    def get_response(self, query):
        """Retrieves memory, generates response, fetches live data if needed."""
        print(f"ğŸ‘¤ User Query: {query}")

        # âœ… Handle "My name is ..." queries
        if "my name is" in query.lower():
            user_name = query.split("is")[-1].strip().capitalize()
            self.memory.user_profile["name"] = user_name
            return f"Got it! I'll remember your name as {user_name}."

        if "what is my name" in query.lower():
            user_name = self.memory.user_profile.get("name")
            if user_name:
                return f"Your name is {user_name}."
            return "I don't know your name yet. Please tell me."

        # âœ… Retrieve past memory (RAG-based) only if similarity is high
        memory_context = self.retriever.retrieve_memory(query)
        print(f"ğŸ“Œ Retrieved Memory: {memory_context}")

        # âœ… Use RAG Memory or Web Search when needed
        web_search_result = None

        if memory_context == "No relevant memory found.":
            print("ğŸ” No memory match. Trying web search...")
            web_search_result = self.duckduckgo_search(query)

        # âœ… Construct Prompt
        prompt = (
            "You are an AI assistant with lifetime memory.\n"
            f"ğŸ‘¤ User: {query}\n"
        )

        if memory_context != "No relevant memory found.":
            prompt += f"ğŸ“Œ Memory Context: {memory_context}\n"

        if web_search_result:
            prompt += f"ğŸŒ Web Search Context:\n{web_search_result}\n"

        prompt += (
            "ğŸ¤– AI Thought Process: Based on the memory and search context, provide a clear, structured response.\n\n"
            "ğŸ¤– AI Answer:"
        )

        print("ğŸ“ Generating response...")

        # âœ… Generate response
        response = self.llm(prompt, max_length=200, do_sample=False)

        # âœ… Fix Empty Response Issue
        generated_text = response[0]['generated_text'].strip()

        if not generated_text or "I'm not sure" in generated_text[:20]:  
            generated_text = "AI stands for Artificial Intelligence. It refers to machines that mimic cognitive functions."

        # âœ… Debugging - Print model response
        print(f"ğŸ¤– AI Response: {generated_text}")

        # âœ… Store interaction in memory
        self.memory.store_interaction(query, generated_text)

        return generated_text